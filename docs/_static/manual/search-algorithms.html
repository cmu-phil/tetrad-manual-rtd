<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Search Algorithms — Tetrad User Guide</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    :root{
      --text:#13151a;
      --muted:#556;
      --bg:#fff;
      --accent:#0a58ca;
      --note:#f6f8ff;
      --warn:#fff7f7;
      --good:#f6fff6;
    }
    html,body{background:var(--bg);color:var(--text);font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.6;margin:0}
    main{max-width:980px;margin:0 auto;padding:28px}
    h1{margin-top:0;line-height:1.2}
    h2,h3{line-height:1.2;margin-top:2rem}
    p.lead{font-size:1.05rem;color:var(--muted)}
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    code{background:#f3f3f3;border-radius:6px;padding:2px 6px}
    .grid{display:grid;gap:14px}
    .twocol{grid-template-columns:1fr 1fr}
    .callout{background:var(--note);border:1px solid #e3e8ff;padding:12px 14px;border-radius:12px;margin:1rem 0}
    .tip{background:var(--good);border:1px solid #d7f5d7;padding:12px 14px;border-radius:12px;margin:1rem 0}
    .caveat{background:var(--warn);border:1px solid #ffd1d1;padding:12px 14px;border-radius:12px;margin:1rem 0}
    ul.tight>li{margin:.2rem 0}
    .alg{padding:14px 16px;border:1px solid #e9e9ef;border-radius:14px;margin:1.2rem 0}
    .alg h3{margin-top:0}
    .badge{display:inline-block;font-size:.85rem;background:#eef;border:1px solid #dde;border-radius:10px;padding:0 8px;margin-right:6px}
    .matrix{background:#fafafa;border:1px solid #eee;border-radius:10px;padding:8px 12px;font-size:.95rem}
  </style>
</head>
<body>
<main>
  <h1>Search Algorithms</h1>
  <p class="lead">This page explains the major discovery algorithms available in Tetrad. It’s written for users of all levels, with plain‑English guidance on when to use each method, what it assumes, and what to watch out for. For knobs and defaults, see the separate <a href="parameter.definitions.txt">Search Parameters</a>.</p>

  <div class="callout">
    <strong>Quick chooser:</strong>
    <ul class="tight">
      <li><strong>No hidden confounders (DAG target):</strong> try <em>FGES</em> for large N; <em>BOSS</em> or <em>GRaSP</em> as alternatives. Prefer <em>PC/PC‑Max</em> if you want test‑based control (α, depth).</li>
      <li><strong>Hidden confounders/selection bias possible (PAG target):</strong> use <em>GFCI</em> (balanced) or <em>FCI/RFCI</em> (baselines). For higher precision/efficiency, try <em>FCIT</em> (targeted testing).</li>
      <li><strong>Small N or unstable results:</strong> enable <em>Bootstrap</em> and inspect edge frequencies in a Graph child.</li>
    </ul>
  </div>

  <h2>Background: outputs & assumptions</h2>
  <ul class="tight">
    <li><strong>DAG/CPDAG:</strong> no latent confounding assumed. Most DAG algorithms return a <em>CPDAG</em> (an equivalence class).</li>
    <li><strong>PAG:</strong> allows for hidden confounders/selection bias. PAGs encode what is identifiable from data + assumptions.</li>
    <li><strong>CI tests vs scores:</strong> Constraint‑based algorithms rely on conditional independence (CI) tests; score‑based ones optimize a goodness‑of‑fit score with penalties.</li>
  </ul>

  <h2>Algorithms</h2>

  <section class="alg" id="fges">
    <h3>FGES — Fast Greedy Equivalence Search <span class="badge">Score‑based</span> <span class="badge">DAG/CPDAG</span></h3>
    <p><strong>What it does:</strong> Greedy hill‑climb over equivalence classes of DAGs to maximize a score (often BIC‑style). It adds/removes edges that most improve the score, efficiently navigating large models.</p>
    <p><strong>When to use:</strong> Medium‑to‑large datasets; you believe there are no major latent confounders; you want speed and a clear objective function.</p>
    <p><strong>Strengths:</strong> Scales well; parallelizable; robust with good scores; clear trade‑off via penalty.</p>
    <p><strong>Limitations:</strong> Assumes no latent confounding; can get stuck in local optima; score choice matters.</p>
    <p><strong>Inputs:</strong> Tabular or covariance data. <strong>Output:</strong> CPDAG.</p>
    <p><strong>Key parameters:</strong> penalty/penaltyDiscount; restarts; number of threads; optional faithfulness or prior settings (see <a href="parameter.definitions.txt">Search Parameters</a>).</p>
    <div class="tip"><strong>Tip:</strong> If orientations seem weak, try a slightly stronger penalty (to reduce spurious adjacencies) or add knowledge (tiers).</div>
  </section>

  <section class="alg" id="boss">
    <h3>BOSS — Best Order Score Search <span class="badge">Score‑based</span> <span class="badge">DAG/CPDAG</span></h3>
    <p><strong>What it does:</strong> Searches over variable orders with a score that evaluates parent sets consistent with each order. Often more robust orientation than pure structure hill‑climbing.</p>
    <p><strong>When to use:</strong> Similar settings as FGES; consider BOSS if you want an order‑search alternative that plays well with hybrid methods.</p>
    <p><strong>Strengths:</strong> Good empirical performance; useful as a guidance score in hybrids (e.g., FCIT, BOSS‑FCI).</p>
    <p><strong>Limitations:</strong> Still assumes no latents for DAG targets; cost grows with p (use threads).</p>
    <p><strong>Inputs/Output:</strong> Tabular or covariance → CPDAG.</p>
    <p><strong>Key parameters:</strong> score choice; order search controls; threads.</p>
  </section>

  <section class="alg" id="grasp">
    <h3>GRaSP — Greedy Relaxations of Sparsest Permutation <span class="badge">Score‑based</span> <span class="badge">DAG/CPDAG</span></h3>
    <p><strong>What it does:</strong> Permutation‑based heuristic that seeks sparse, high‑score structures by relaxing permutation constraints.</p>
    <p><strong>When to use:</strong> As an alternative to FGES/BOSS; compare results to gauge robustness.</p>
    <p><strong>Strengths:</strong> Can handle challenging landscapes; different bias than FGES can help triangulate structure.</p>
    <p><strong>Limitations:</strong> Heuristic; tune carefully; may be slower for very large p without threading.</p>
    <p><strong>Inputs/Output:</strong> Tabular or covariance → CPDAG.</p>
  </section>

  <section class="alg" id="pc">
    <h3>PC — Constraint‑based (with CI tests) <span class="badge">Constraint‑based</span> <span class="badge">DAG/CPDAG</span></h3>
    <p><strong>What it does:</strong> Starts with a complete undirected graph and removes edges when a CI test says two variables are independent given some conditioning set; then orients remaining edges by graph rules.</p>
    <p><strong>When to use:</strong> You want explicit statistical control via CI tests and α levels; data are moderate size; assumptions match your CI test (e.g., Fisher‑Z for Gaussian).</p>
    <p><strong>Strengths:</strong> Transparent; α gives a clear knob; supports many CI tests (continuous, discrete, kernel‑based).</p>
    <p><strong>Limitations:</strong> Sensitive to CI test specification and sample size; depth limits matter for runtime and correctness.</p>
    <p><strong>Inputs/Output:</strong> Tabular → CPDAG.</p>
    <p><strong>Key parameters:</strong> α (significance), depth, CI test type, threads.</p>
  </section>

  <section class="alg" id="pcmax">
    <h3>PC‑Max — Aggressive orientation variant <span class="badge">Constraint‑based</span> <span class="badge">DAG/CPDAG</span></h3>
    <p><strong>What it does:</strong> A variant of PC that prioritizes stronger separation sets to reduce orientation conflicts and spurious edges.</p>
    <p><strong>When to use:</strong> You prefer higher precision (fewer false positives) even if recall drops slightly, especially in noisy settings.</p>
    <p><strong>Inputs/Output:</strong> Tabular → CPDAG. <strong>Parameters:</strong> same family as PC.</p>
  </section>

  <section class="alg" id="fci">
    <h3>FCI — Fast Causal Inference <span class="badge">Constraint‑based</span> <span class="badge">PAG</span></h3>
    <p><strong>What it does:</strong> Extends constraint‑based discovery to settings with latent confounding and selection bias. Produces a <em>PAG</em>, which encodes what is identifiable under those possibilities.</p>
    <p><strong>When to use:</strong> Whenever hidden confounders may exist or you explicitly want PAGs.</p>
    <p><strong>Strengths:</strong> Canonical baseline for latent‑variable discovery; widely used and studied.</p>
    <p><strong>Limitations:</strong> Potentially many CI tests; can be conservative with limited data; tuning depth/α is important.</p>
    <p><strong>Inputs/Output:</strong> Tabular → PAG. <strong>Key parameters:</strong> α, depth, max path length, CI test type, threads.</p>
  </section>

  <section class="alg" id="rfci">
    <h3>RFCI — Really Fast Causal Inference <span class="badge">Constraint‑based</span> <span class="badge">PAG</span></h3>
    <p><strong>What it does:</strong> A faster, more pruning‑oriented FCI variant. Trades some completeness for speed.</p>
    <p><strong>When to use:</strong> Larger p or N when FCI is too slow; as a first pass to get a sense of structure.</p>
    <p><strong>Inputs/Output:</strong> Tabular → PAG. <strong>Parameters:</strong> similar to FCI.</p>
  </section>

  <section class="alg" id="gfci">
    <h3>GFCI — Greedy Fast Causal Inference <span class="badge">Hybrid</span> <span class="badge">PAG</span></h3>
    <p><strong>What it does:</strong> Combines score‑based search (e.g., FGES) with FCI‑style constraint pruning/orientation for PAG discovery. Often a strong default when latents are possible.</p>
    <p><strong>When to use:</strong> You want a pragmatic balance of precision/recall and runtime with possible latents.</p>
    <p><strong>Strengths:</strong> Fewer tests than FCI; leverages score signal; good empirical accuracy.</p>
    <p><strong>Limitations:</strong> Behavior depends on score settings; still conservative with very weak signal.</p>
    <p><strong>Inputs/Output:</strong> Tabular → PAG.</p>
  </section>

  <section class="alg" id="fcit">
    <h3>FCIT — FCI with Targeted Testing <span class="badge">Hybrid</span> <span class="badge">PAG</span></h3>
    <p><strong>What it does:</strong> Uses a score (e.g., BOSS) to <em>prioritize</em> which CI tests matter most, avoiding the exhaustive testing in vanilla FCI. That usually improves precision and efficiency.</p>
    <p><strong>When to use:</strong> Medium‑to‑large problems where FCI’s exhaustive testing is unstable or too slow; when you value precision.</p>
    <p><strong>Strengths:</strong> Targeted tests; fewer spurious independences; better runtime.</p>
    <p><strong>Limitations:</strong> Requires a score; behavior depends on guidance quality and thresholds.</p>
    <p><strong>Inputs/Output:</strong> Tabular → PAG. <strong>Parameters:</strong> CI test + score settings; testing budget/priority controls.</p>
  </section>

  <section class="alg" id="boss-fci">
    <h3>BOSS‑FCI / GRaSP‑FCI — Hybrid variants <span class="badge">Hybrid</span> <span class="badge">PAG</span></h3>
    <p><strong>What it does:</strong> Replace FGES with BOSS or GRaSP inside the GFCI/FCI hybrid template. Can shift precision/recall and runtime in useful ways.</p>
    <p><strong>When to use:</strong> If GFCI works but you want to stress‑test robustness to the score engine.</p>
  </section>

  <section class="alg" id="cam">
    <h3>CAM — Causal Additive Models <span class="badge">Additive‑noise</span> <span class="badge">DAG</span></h3>
    <p><strong>What it does:</strong> Assumes variables follow additive‑noise relations with smooth functions, then estimates a DAG consistent with those models.</p>
    <p><strong>When to use:</strong> Nonlinear continuous data where additive‑noise is plausible and no major latents are expected.</p>
    <p><strong>Limitations:</strong> Sensitive to modeling choices and noise assumptions; not designed for latent confounding.</p>
  </section>

  <section class="alg" id="gin">
    <h3>GIN / Related Independent‑noise approaches <span class="badge">Independent‑noise</span> <span class="badge">DAG</span></h3>
    <p><strong>What it does:</strong> Looks for directions where residuals are independent, exploiting asymmetries in cause→effect vs effect→cause fits.</p>
    <p><strong>When to use:</strong> Small to medium problems to orient ambiguous pairs under independent‑noise assumptions.</p>
  </section>

  <h2>Choosing CI tests & scores</h2>
  <div class="matrix">
    <ul class="tight">
      <li><strong>Continuous (Gaussian-ish):</strong> Fisher‑Z CI; SEM‑BIC score.</li>
      <li><strong>Discrete (nominal):</strong> G‑test or Chi‑square; BDeu/BIC‑style scores.</li>
      <li><strong>Mixed / Nonlinear:</strong> KCI or RCIT for CI (more costly); consider discretization or basis‑function approaches before resorting to kernels.</li>
      <li><strong>Covariance‑only:</strong> Use algorithms/scores that accept covariance + N (e.g., FGES with SEM‑BIC).</li>
    </ul>
  </div>

  <h2>Common pitfalls & how to fix them</h2>
  <ul class="tight">
    <li><strong>Too many edges (dense graph):</strong> Increase penalty (scores) or lower α (CI tests); add tiers in Knowledge; enable bootstrap and keep stable edges.</li>
    <li><strong>Too few edges (empty graph):</strong> Decrease penalty or raise α; check data preprocessing (variance‑zero columns, scaling); ensure depth isn’t too small.</li>
    <li><strong>Weird orientations:</strong> Verify CI test assumptions; use PC‑Max; add minimal knowledge (tiers) to enforce direction.</li>
    <li><strong>Runtime is high:</strong> Limit depth; switch to FCIT or RFCI; use threads; pre‑screen variables (domain‑driven pruning).</li>
  </ul>

  <h2>Where to find parameters</h2>
  <p>All knobs and default values live in the <a href="parameter.definitions.txt">Search Parameters</a> file (alphabetized, verbatim definitions). Link this page from the Search Box under “Algorithm chooser.”</p>

</main>
</body>
</html>
