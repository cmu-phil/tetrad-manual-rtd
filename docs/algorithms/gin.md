# GIN (Generalized Independent Noise)

**Type:** Latent-variable / constraint-based  
**Output:** Latent DAG over observed and latent variables (directed edges, with undirected tail–tail edges allowed among some latent nodes)

GIN (“Generalized Independent Noise”) is a latent-variable causal discovery algorithm for Linear, Non-Gaussian Latent variable Models (LiNGLaMs). It uses the GIN condition to locate groups of observed variables that share latent causes, then infers a causal order among those latent variables and builds a latent DAG that explains the observed indicators.  [oai_citation:0‡arXiv](https://arxiv.org/abs/2010.04917?utm_source=chatgpt.com)

The current implementation corresponds to the practical GIN clustering and root-peeling procedure of Xie et al. (2024), applied to a single latent layer over observed variables; the full LaHiCaSl hierarchical extension is not yet implemented in Tetrad.

---

## Key Idea

GIN is built around the GIN condition: for two sets of observed variables \(Y\) and \(Z\), there exists a non-zero vector \(\omega\), determined from the cross-covariance of \(Y\) and \(Z\), such that the scalar surrogate \(E_{Y\mid Z} = \omega^\top Y\) is independent of \(Z\) if and only if certain latent causal relations hold.  [oai_citation:1‡NeurIPS Proceedings](https://proceedings.neurips.cc/paper/2020/file/aa475604668730af60a0a87cc92604da-Paper.pdf?utm_source=chatgpt.com)  The Tetrad implementation follows the recursive learning procedure from the NeurIPS/JMLR papers:

1. **GIN-based clustering (Algorithm 1):**  
   It searches over subsets of observed variables of increasing size and tests whether a candidate set \(Y\) satisfies the GIN condition relative to the remaining variables \(Z\). Sets that pass are merged into clusters, each interpreted as a group of pure or nearly pure indicators of a latent variable.

2. **Root-peeling causal ordering (Algorithm 2):**  
   Using a “half-split” construction on each cluster, GIN repeatedly identifies root latent sets—clusters whose latent variable is causally earliest with respect to the others—by checking appropriate GIN tests. These root sets are peeled off one by one to produce a causal order over latent variables.

3. **Latent graph construction:**  
   Tetrad creates one latent node per cluster, connects earlier latents to later latents according to the learned order, attaches each latent to its observed indicators, and connects any remaining unordered latents with tail–tail undirected edges.

---

## When to Use

- You believe your data are generated by a **linear, non-Gaussian acyclic causal model** with **latent variables** (LiNGLaM / LiNGLaH setting).  [oai_citation:2‡Journal of Machine Learning Research](https://www.jmlr.org/papers/v25/23-1052.html?utm_source=chatgpt.com)
- Each latent variable has (approximately) **at least two pure indicators** (double-pure measurement assumption), so that clusters of indicators are meaningful.  [oai_citation:3‡NeurIPS Proceedings](https://proceedings.neurips.cc/paper/2020/file/aa475604668730af60a0a87cc92604da-Paper.pdf?utm_source=chatgpt.com)
- You want to **explicitly recover latent variables and their causal ordering**, not just a causal graph over observed variables.
- Your observed variables are **continuous** and you can afford nonparametric independence tests (KCI/HSIC-style) at moderate dimensionality.
- You are comfortable with stronger assumptions (non-Gaussian noise, linear relations at the latent level) in exchange for **stronger identifiability** of the latent structure.

---

## Prior Knowledge Support

**Does it accept background knowledge?**  
No. The current Tetrad implementation of GIN does **not** take background knowledge (forbidden/required edges, tiers, etc.) into account. The algorithm is entirely data-driven: clusters and latent ordering are determined solely by GIN tests on the dataset.

---

## Strengths

- **Explicit latent structure:** Recovers latent variables as nodes in the output graph, not just as implicit confounders.
- **Identifiability under realistic assumptions:** Under LiNGLaM assumptions with non-Gaussian noise and appropriate measurement structure, the latent causal graph is identifiable.  [oai_citation:4‡Journal of Machine Learning Research](https://www.jmlr.org/papers/v25/23-1052.html?utm_source=chatgpt.com)
- **Causal ordering of latents:** Provides a causal order over latent variables (and hence directional information among them), not just undirected latent blocks.
- **Modern theory:** Based on the GIN condition, which generalizes the independent noise condition and connects directly to graphical criteria.  [oai_citation:5‡NeurIPS Proceedings](https://proceedings.neurips.cc/paper/2020/file/aa475604668730af60a0a87cc92604da-Paper.pdf?utm_source=chatgpt.com)

---

## Limitations

- **Model assumptions are strong:** Requires linear structural relations at the latent level, non-Gaussian i.i.d. errors, and a double-pure measurement structure; violations can degrade performance or break identifiability.  [oai_citation:6‡Journal of Machine Learning Research](https://www.jmlr.org/papers/v25/23-1052.html?utm_source=chatgpt.com)
- **Computational cost:** The clustering step enumerates subsets of observed variables, which becomes expensive for large numbers of variables or large cluster sizes.
- **Continuous, reasonably scaled data:** Designed for continuous variables with meaningful covariance structure; it is not directly suitable for purely discrete data without additional preprocessing.
- **Limited background-knowledge integration:** There is currently no way to constrain or guide the latent structure with prior knowledge inside GIN.

---

## Key Parameters in Tetrad

All parameters appear in the GUI (camelCase form) and in scripting interfaces.

| Parameter (camelCase) | Description |
|------------------------|-------------|
| `alpha`   | Significance level used for GIN-based independence decisions. At each GIN test, p-values for surrogate–Z independence are combined via Fisher’s method; clusters and root sets are accepted when the combined p-value is at least `alpha`. Lower values make the algorithm more conservative. |
| `verbose` | If `true`, Tetrad logs additional information about intermediate clusters, the learned latent causal order, and any unordered clusters, useful for debugging or inspection. |

*(Parameters for the independence test itself—e.g., kernel type, bandwidth—are configured separately in the chosen independence-test object and are not part of GIN’s own parameter list.)*

---

## Reference

Xie, F., Huang, B., Chen, Z., Cai, R., Glymour, C., Geng, Z., & Zhang, K. (2024).  
**Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables.** *Journal of Machine Learning Research, 25*, 1–61.  [oai_citation:7‡Journal of Machine Learning Research](https://www.jmlr.org/papers/v25/23-1052.html?utm_source=chatgpt.com)

(Original conference version: Xie, F., Cai, R., Huang, B., Glymour, C., Hao, Z., & Zhang, K. (2020).  
**Generalized Independent Noise Condition for Estimating Latent Variable Causal Graphs.** *NeurIPS*.  [oai_citation:8‡NeurIPS Proceedings](https://proceedings.neurips.cc/paper/2020/file/aa475604668730af60a0a87cc92604da-Paper.pdf?utm_source=chatgpt.com))

---

## Summary

GIN is a latent-variable causal discovery algorithm that uses the Generalized Independent Noise condition to simultaneously discover clusters of indicators, recover latent variables, and infer a causal ordering among them—giving you an explicit latent DAG when LiNGLaM assumptions and measurement conditions are reasonably met.  [oai_citation:9‡algorithm.template.md](sediment://file_00000000e37071fd944527e7dafaa5ad)