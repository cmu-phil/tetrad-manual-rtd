<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Compare Box — Tetrad User Guide</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.6;padding:2rem;max-width:900px;margin:auto;}
    h1,h2,h3{line-height:1.2}
    .callout{background:#f6f8ff;border:1px solid #e3e8ff;padding:12px 14px;border-radius:12px;margin:1rem 0}
    .caveat{background:#fff7f7;border:1px solid #ffd1d1;padding:12px 14px;border-radius:12px;margin:1rem 0}
    .grid{display:grid;grid-template-columns:1fr 1fr;gap:14px}
    ol{margin-top:.4rem}
    .small{font-size:.95rem;color:#555}
    a{color:#0a58ca;text-decoration:none}
    a:hover{text-decoration:underline}
    code{background:#f3f3f3;border-radius:6px;padding:2px 6px}
    .metric{background:#f7fff7;border:1px solid #d7f7d7;border-radius:10px;padding:8px 10px;margin:.5rem 0}
  </style>
</head>
<body>

  <h1>Compare Box</h1>
  <p class="small">User-facing instructions. For parameter definitions, see the external <a href="parameter.definitions.txt">Parameter Reference</a>.</p>

  <h2>What it’s for</h2>
  <p>Compare two or more graphs (estimated vs. true, or between algorithms) and compute edge/arrowhead metrics, misclassifications, intersections, and fit statistics.</p>

  <h2>When to use it</h2>
  <ul>
    <li>Evaluate a search output against a ground-truth graph.</li>
    <li>Compare different algorithms or parameterizations on the same dataset.</li>
    <li>Inspect where disagreements occur (endpoint-level confusions).</li>
  </ul>

  <h2>Inputs → Outputs</h2>
  <ul>
    <li><strong>Inputs:</strong> One or more graphs from Graph/Search/PM/IM boxes (at least 2).</li>
    <li><strong>Outputs:</strong> Tables and summaries of agreement metrics; optional derived graphs (e.g., intersections).</li>
  </ul>

  <h2>How to use</h2>
  <ol>
    <li><strong>Add graphs:</strong> Link two or more parent boxes that each produce a graph.</li>
    <li><strong>Select comparison mode:</strong> Edgewise, Arrowheads, Misclassifications, Intersections, Fit.</li>
    <li><strong>Run:</strong> Click <em>Compare</em> to compute tables and (optionally) export CSV.</li>
  </ol>

  <h2>Common tasks / recipes</h2>
  <h3>Edgewise metrics (precision/recall/F1)</h3>
  <ol>
    <li>Select <em>Edgewise</em> mode.</li>
    <li>Choose a target graph as “reference” (if applicable).</li>
    <li>Inspect precision, recall, F1. Optionally export CSV.</li>
  </ol>
  <div class="metric"><strong>Interpretation:</strong> High precision = few spurious adjacencies; high recall = few missed adjacencies.</div>

  <h3>Arrowhead metrics (orientation accuracy)</h3>
  <ol>
    <li>Select <em>Arrowheads</em>.</li>
    <li>Review arrowhead precision/recall; use with caution on sparse data.</li>
  </ol>
  <div class="metric"><strong>Interpretation:</strong> High arrowhead precision/recall indicates correct orientation of causal directions among shared adjacencies.</div>

  <h3>Misclassifications (endpoint confusions)</h3>
  <ol>
    <li>Select <em>Misclassifications</em>.</li>
    <li>Review the endpoint confusion matrix (e.g., true tail vs estimated circle, etc.).</li>
  </ol>
  <div class="metric"><strong>Use case:</strong> Pinpoint whether errors come from missing adjacencies vs. wrong endpoints.</div>

  <h3>Intersections (consensus structure)</h3>
  <ol>
    <li>Select <em>Intersections</em>.</li>
    <li>Optionally filter by edge type or endpoint certainty to find stable structure across algorithms or bootstraps.</li>
  </ol>
  <div class="metric"><strong>Use case:</strong> Consensus graph for reporting or for downstream parameter estimation.</div>

  <h3>Fit statistics</h3>
  <ol>
    <li>Select <em>Fit</em>.</li>
    <li>Provide data and (optionally) an estimator/IM parent.</li>
    <li>Inspect fit indices (e.g., likelihood, BIC conventions for Tetrad vs. standard).</li>
  </ol>
  <div class="caveat"><strong>Note:</strong> Tetrad’s BIC convention may differ (higher-is-better in some modules). Check the per-module note when comparing values.</div>

  <h2>Caveats & limitations</h2>
  <ul>
    <li>Arrowhead metrics require shared adjacencies; weak adjacencies can deflate orientation scores.</li>
    <li>Fit stats depend on available data and appropriate estimator settings.</li>
    <li>When comparing PAGs vs DAGs, ensure compatible edge encodings.</li>
  </ul>

  <h2>Related boxes</h2>
  <ul>
    <li><strong>Graph</strong> → prepare or clean graphs before comparison.</li>
    <li><strong>Search</strong> → produce graphs to compare.</li>
    <li><strong>Parametric / Instantiated Model</strong> → for fit statistics with data.</li>
  </ul>

  <h2>Where to find parameters</h2>
  <p>See the separate <a href="parameter.definitions.txt">Parameter Reference</a> for any Compare-specific toggles (if present).</p>

</body>
</html>
